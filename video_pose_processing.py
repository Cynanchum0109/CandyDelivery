import cv2
from ultralytics import YOLO
import numpy as np
import time

# Some parts of the feature calculation is initially generated by AI, then modified
# to fit the project needs.

VIDEO_FILE_PATH = "WIN_20251107_13_47_54_Pro.mp4"
VIDEO_OUTPUT_PATH = "analyze1_video_yolo.mp4"

print("Loading YOLOv8-Pose model...")
model = YOLO('yolov8n-pose.pt')
print("Model loaded successfully.")

KEYPOINT_NAMES = {
    0: 'nose',
    1: 'left_eye', 2: 'right_eye',
    3: 'left_ear', 4: 'right_ear',
    5: 'left_shoulder', 6: 'right_shoulder',
    7: 'left_elbow', 8: 'right_elbow',
    9: 'left_wrist', 10: 'right_wrist',
    11: 'left_hip', 12: 'right_hip',
    13: 'left_knee', 14: 'right_knee',
    15: 'left_ankle', 16: 'right_ankle'
}
FACE_KEYPOINTS = [0, 1, 2, 3, 4]
WRIST_KEYPOINTS = [9, 10]

COLOR_PERSON_BOX = (0, 255, 0)   
COLOR_SKELETON = (255, 0, 0)  
COLOR_FACE_BOX = (255, 0, 255)  
COLOR_DISENGAGED = (0, 0, 255)    
COLOR_WRIST = (0, 0, 255)      

def main():
    cap = cv2.VideoCapture(VIDEO_FILE_PATH)

    if not cap.isOpened():
        print(f"Error: Could not open video file: {VIDEO_FILE_PATH}")
        return

    LOG_FILE_PATH = VIDEO_OUTPUT_PATH.replace(".mp4", "_log.csv")
    try:
        log_file = open(LOG_FILE_PATH, 'w')
        log_file.write("frame,timestamp_ms,person_id,slouch_ratio,is_slouching,head_turn_ratio,head_tilt_ratio,face_visibility_score,shoulder_width_ratio,arm_cross_metric,nose_velocity,face_detected,wrist_detected\n")
        print(f"Will save analysis data to: {LOG_FILE_PATH}")
    except Exception as e:
        print(f"Error: Could not open log file {LOG_FILE_PATH} for writing: {e}")
        return
    
    try:
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))

        if not video_writer.isOpened():
            print(f"Error: Could not open video writer for path: {VIDEO_OUTPUT_PATH}")
            cap.release()
            return

        print(f"Video properties: {frame_width}x{frame_height} @ {fps:.2f} FPS")
        print(f"Will save processed video to: {VIDEO_OUTPUT_PATH}")

    except Exception as e:
        print(f"Error setting up video writer: {e}")
        cap.release()
        log_file.close()
        return

    frame_count = 0
    ret, frame = cap.read()

    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    last_kpts_person_0 = None

    print("Starting video analysis (this may take a while)...")

    while ret:
        lab_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
        l_channel, a_channel, b_channel = cv2.split(lab_frame)

        l_channel_enhanced = clahe.apply(l_channel)
        lab_enhanced = cv2.merge((l_channel_enhanced, a_channel, b_channel))

        enhanced_frame = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)
        timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)

        results = model(enhanced_frame, verbose=False)

        annotated_frame = results[0].plot(img=frame)
        if results[0].keypoints is not None:

            for i, person_kpts in enumerate(results[0].keypoints.cpu().data):

                # Initialize analysis variables for logging
                slouch_ratio = np.nan
                is_slouching = False
                face_detected = False
                wrist_detected = False
                head_turn_ratio = np.nan
                arm_cross_metric = np.nan
                # Initialize new feature variables
                head_tilt_ratio = np.nan
                face_visibility_score = 0
                shoulder_width_ratio = np.nan
                nose_velocity = 0.0 # Default to 0 (no movement)

                l_shoulder_conf = person_kpts[5][2]
                r_shoulder_conf = person_kpts[6][2]
                l_hip_conf = person_kpts[11][2]
                r_hip_conf = person_kpts[12][2]

                # Only proceed if all 4 keypoints are confidently detected
                if l_shoulder_conf > 0.6 and r_shoulder_conf > 0.6 and l_hip_conf > 0.6 and r_hip_conf > 0.6:

                    # Get the corresponding bounding box for this person
                    box = results[0].boxes.data[i]
                    person_height_pixels = box[3] - box[1]

                    # Get Y-coordinates
                    l_shoulder_y = person_kpts[5][1]
                    r_shoulder_y = person_kpts[6][1]
                    l_hip_y = person_kpts[11][1]
                    r_hip_y = person_kpts[12][1]

                    # Calculate average Y-position for shoulders and hips
                    avg_shoulder_y = (l_shoulder_y + r_shoulder_y) / 2
                    avg_hip_y = (l_hip_y + r_hip_y) / 2

                    # Calculate vertical distance (torso height in pixels)
                    torso_height_pixels = abs(avg_hip_y - avg_shoulder_y)

                    if person_height_pixels > 0:
                        slouch_ratio = torso_height_pixels / person_height_pixels

                        # Calculate Shoulder Width Ratio
                        l_shoulder_x = person_kpts[5][0]
                        r_shoulder_x = person_kpts[6][0]
                        shoulder_width_pixels = abs(l_shoulder_x - r_shoulder_x)
                        shoulder_width_ratio = shoulder_width_pixels / person_height_pixels
                    else:
                        slouch_ratio = np.nan
                        shoulder_width_ratio = np.nan

                    # Define a threshold for slouching
                    SLOUCH_THRESHOLD = 0.25

                    if slouch_ratio < SLOUCH_THRESHOLD:
                        is_slouching = True # Log this
                        # Draw a "Slouching" label
                        label = "SLOUCHING (Disengaged?)"
                        box_x1 = int(box[0])
                        box_y1 = int(box[1])
                        cv2.putText(annotated_frame, label, (box_x1, box_y1 - 30),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, COLOR_DISENGAGED, 2)

                nose_conf = person_kpts[0][2]
                l_eye_conf = person_kpts[1][2]
                r_eye_conf = person_kpts[2][2]
                l_ear_conf = person_kpts[3][2]
                r_ear_conf = person_kpts[4][2]

                # Calculate Face Visibility Score
                face_confs = [nose_conf, l_eye_conf, r_eye_conf, l_ear_conf, r_ear_conf]
                face_visibility_score = sum(1 for conf in face_confs if conf > 0.5)

                if nose_conf > 0.6 and l_eye_conf > 0.6 and r_eye_conf > 0.6:
                    nose_x, nose_y = person_kpts[0][:2]
                    l_eye_x, l_eye_y = person_kpts[1][:2]
                    r_eye_x, r_eye_y = person_kpts[2][:2]

                    # Head Turn Ratio
                    eye_center_x = (l_eye_x + r_eye_x) / 2
                    eye_center_y = (l_eye_y + r_eye_y) / 2

                    dist_nose_to_eye_center = np.linalg.norm([nose_x - eye_center_x, nose_y - eye_center_y])
                    dist_between_eyes = np.linalg.norm([l_eye_x - r_eye_x, l_eye_y - r_eye_y])

                    if dist_between_eyes > 0:
                        head_turn_ratio = dist_nose_to_eye_center / dist_between_eyes

                    # Head Tilt Ratio
                    if 'person_height_pixels' in locals() and person_height_pixels > 0:
                        dist_nose_to_eye_vertical = nose_y - eye_center_y
                        # Positive value = nose below eyes (tilted down)
                        head_tilt_ratio = dist_nose_to_eye_vertical / person_height_pixels

                if i == 0: # Only track person 0 for simplicity
                    if last_kpts_person_0 is not None and nose_conf > 0.5:
                        # Calculate Nose Velocity
                        last_nose_pos = last_kpts_person_0[0][:2]
                        current_nose_pos = person_kpts[0][:2]

                        # Get pixel distance
                        nose_velocity = np.linalg.norm(current_nose_pos - last_nose_pos)

                    # Update last keypoints for next frame
                    last_kpts_person_0 = person_kpts

                l_wrist_conf = person_kpts[9][2]
                r_wrist_conf = person_kpts[10][2]

                if l_wrist_conf > 0.5 and r_hip_conf > 0.5:
                    l_wrist_pos = person_kpts[9][:2]
                    r_hip_pos = person_kpts[12][:2]
                    dist_l_wrist_r_hip = np.linalg.norm(l_wrist_pos - r_hip_pos)
                    arm_cross_metric = dist_l_wrist_r_hip

                if r_wrist_conf > 0.5 and l_hip_conf > 0.5:
                    r_wrist_pos = person_kpts[10][:2]
                    l_hip_pos = person_kpts[11][:2]
                    dist_r_wrist_l_hip = np.linalg.norm(r_wrist_pos - l_hip_pos)

                    # If we have both distances, use the smallest one (strongest signal)
                    if arm_cross_metric is np.nan or dist_r_wrist_l_hip < arm_cross_metric:
                        arm_cross_metric = dist_r_wrist_l_hip

                # Highlight Wrists (Gesture)
                for kpt_index in WRIST_KEYPOINTS:
                    x, y, conf = person_kpts[kpt_index]
                    if conf > 0.5: # Only draw if confidence is high
                        wrist_detected = True
                        cv2.circle(annotated_frame, (int(x), int(y)), 8, COLOR_WRIST, -1) # Solid red circle

                # Draw Face Bounding Box (for Emotion cue)
                face_points = []
                for kpt_index in FACE_KEYPOINTS:
                    x, y, conf = person_kpts[kpt_index]
                    if conf > 0.5:
                        face_points.append((int(x), int(y)))

                if len(face_points) > 0:
                    face_detected = True # Log this
                    x_coords = [p[0] for p in face_points]
                    y_coords = [p[1] for p in face_points]
                    x1, y1 = min(x_coords), min(y_coords)
                    x2, y2 = max(x_coords), max(y_coords)

                    x1 = max(0, x1 - 20)
                    y1 = max(0, y1 - 40)
                    x2 = min(frame_width, x2 + 20)
                    y2 = min(frame_height, y2 + 20)

                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), COLOR_FACE_BOX, 2)
                    cv2.putText(annotated_frame, "Face (Cue for Emotion)", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, COLOR_FACE_BOX, 2)

                log_line = (
                    f"{frame_count},{timestamp_ms:.2f},{i},"
                    f"{slouch_ratio:.4f},{is_slouching},"
                    f"{head_turn_ratio:.4f},{head_tilt_ratio:.4f},"
                    f"{face_visibility_score},{shoulder_width_ratio:.4f},"
                    f"{arm_cross_metric:.2f},{nose_velocity:.2f},"
                    f"{face_detected},{wrist_detected}\n"
                )
                log_file.write(log_line)

        # Write the annotated frame to the output file
        video_writer.write(annotated_frame)

        if frame_count % 30 == 0:
             print(f"Processed frame {frame_count}...")

        frame_count += 1

        # Read the next frame
        ret, frame = cap.read()

        if results[0].keypoints is None:
            last_kpts_person_0 = None

    cap.release()
    video_writer.release()
    log_file.close() # Close the log file
    print("---------------------------------")
    print("Video processing finished!")
    print(f"Output video saved to: {VIDEO_OUTPUT_PATH}")
    print(f"Output log saved to: {LOG_FILE_PATH}")
    print("---------------------------------")

if __name__ == "__main__":
    main()